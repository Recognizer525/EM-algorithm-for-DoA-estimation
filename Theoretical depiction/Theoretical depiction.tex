%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  coding page = utf-8
%%  Статья в ИП-2026
%%  Задача оценивания угловых координат
%%   неподвижных объектов по наблюдениям с пропусками.
%%  Лебедев М.В., Семенихин К.В., Суржиков Г.Ф.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[ams,yap]{iitpinfo} 
\usepackage{hyperref}

\usepackage[english,russian]{babel}
\usepackage[utf8]{inputenc} 
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{physics}
\usepackage{comment}
\usepackage{tikz}
\usepackage{float}
\usepackage[mathscr]{euscript}
\usepackage{cite}
\usepackage{amsfonts}
 
\def\Mexp{{\mathsf M}}
\def\var{{\mathsf D}}
\def\cov{{\mathsf{cov}}}
\def\bA{{\mathbf{A}}}
\def\bH{{\mathbf{H}}}
\def\bR{{\hat{\mathbf{R}}}}
\def\bGamma{{\mathbf{\Gamma}}}
\def\bLambda{{\mathbf{\Lambda}}}
\def\CN{{\mathcal{CN}}}
\def\Info{{\mathcal{I}}}

\def\Natural{{\mathbb{N}}}
\def\Real{{\mathbb{R}}}
\def\Complex{{\mathbb{C}}}
\def\Int{{\mathbb{Z}}}

\def\Var{{\mathcal{D}}}
\def\Norm{{\mathcal{N}}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\Det}{Det}

\begin{document}

\VolumeNo{XX}
\IssueNo{X}
\YearOfIssue{2026}
\setcounter{page}{1}
\CopyrightYear{2026}

\sloppy 

\title{Задача оценивания угловых координат неподвижных объектов по наблюдениям с пропусками.}
\author{М.~В.~Лебедев$^{*,a}$, К.~В.~Семенихин$^{*,b}$, Г.~Ф.~Суржиков$^{*,c}$}

\institute{%
$^{*}$Московский авиационный институт (национальный исследовательский университет),\\ Москва, Россия\\
e-mail: $^{a}$max$\_$max82@inbox.ru, $^{c}$siemenkv@gmail.com, $^{c}$gfsurzhikov@gmail.com}


\received{Поступила в редколлегию XX.XX.2026}

\titlerunning{Оценивание угловых координат по наблюдениям с пропусками}
\authorrunning{ЛЕБЕДЕВ, СЕМЕНИХИН, СУРЖИКОВ}
\CopyrightedAuthors{Лебедев, Семенихин, Суржиков}
\Rubric{теория и методы обработки информации}

\maketitle 

\begin{abstract}
В~работе построен ЕМ-алгоритм для оценивания угловых координат неподвижных объектов при наличии пропусков в наблюдениях, полученных линейной равномерной антенной решеткой. Пропуски в наблюдениях рассматриваются как результат стохастических аппаратных сбоев. В качестве скрытых переменных рассматриваются как пропуски в наблюдениях, так и исходные сигналы. Предложенный алгоритм способен находить оптимальные оценки угловых координат при достаточно хорошей инициализации и малом числе источников сигналов.
\end{abstract}

\textbf{КЛЮЧЕВЫЕ СЛОВА:} 
оценивание угловых координат, ЕМ-алгоритм, наблюдения с пропусками, линейная равномерная антенная решетка, оценка максимального правдоподобия.

\section{Введение}
В парадигме статистической обработки сигналов особое место занимает задача оценивания угловых координат (DoA) источников сигналов~\cite{KrimViberg, Godara}. Существуют две основные категории методов для оценивания угловых координат: подпространственные (к примеру, CAPON, MUSIC, ESPRIT), основанные на разбиении ковариационной матрицы наблюдений на подпространство, соответствующее сигналу, и подпространство, соответствующее шуму, и методы, основанные на поиске ММП-оценки. Последние оказываются особенно полезными в постановках задач, предполагающих относительно низкое отношение сигнал-шум, корреляцию между сигналами или малое число наблюдений.  

Одним из классических подходов к поиску оценки максимального правдоподобия в моделях с неполными наблюдениями является ЕМ-алгоритм~\cite{DempsterRubin}. В контексте задач оценивания угловых координат источников скрытые переменные позволяют переформулировать задачу таким образом, что применение ЕМ-алгоритма становится естественным и вычислительно удобным. В практических приложениях зачастую используются модификации этого алгоритма, позволяющие упростить оптимизацию на М-шаге (ECM-алгоритм, ~\cite{MengRubin}), либо обеспечивающие более быструю сходимость (SAGE-алгоритм, ~\cite{FesslerHero}), и подобные методы уже применяются для оценивания угловых координат источников ~\cite{MillerFuhrmann, FederWeinstein, GongLyu2021}. Данные алгоритмы используют предположение об известной ковариационной матрице шума. Активное развитие получили методы, использующие ЕМ-алгоритм в условиях априорной неопределенности ковариационной матрицы шума ~\cite{GongLyu2023, Schwarz, GongLyu2023second}. В то время как в вышеприведенных работах используется пространственная обработка сигналов, в ~\cite{Xunxue} предлагается подход на основе временной обработки сигналов с использованием ЕМ-алгоритма, в роли скрытых переменных рассматриваются систематические ошибки измерения разностей времени прихода сигнала (TDoA). В ~\cite{Wen} построен ЕМ-алгоритм для оценивания угловых координат и иных характеристик сигналов для беспроводной сенсорной сети.

В реальных условиях эксплуатации сенсоры могут быть подвержены стохастическим аппаратным сбоям, вследствие чего их показания в определенные моменты времени могут не быть надежными. В ~\cite{Gimenez-Febrer} ЕМ-алгоритм используется для оценивания характеристик сигнала в беспроводной сенсорной сети, причем алгоритм также оценивает исправность узлов этой сети.

В работе формулируется задача поиска оптимальных оценок угловых координат неподвижных  источников сигналов в условиях, когда часть сенсоров подвержены аппаратным сбоям или случайным помехам, и, ввиду этого, часть наблюдений содержит случайные по своему происхождению пропуски. Используется методология, основанная на представлении сигнала и шума в комплексной форме, сигнал и шум предполагаются дискретными независимыми комплексными гауссовскими процессами с независимыми одинаково распределенными отсчетами.  В качестве скрытых переменных рассматриваются исходные сигналы и пропуски в наблюдениях. Ввиду чувствительности ЕМ-алгоритма к инициализации, используется мультистарт, для первой итерации используется оценка, полученная алгоритмом MUSIC, на последующих - случайные смещения указанной оценки.  В условиях, когда число сенсоров оказывается меньше числа наблюдений, причем от трети и более сенсоров ненадежны, оценки, полученные с помощью алгоритма MUSIC оказываются существенно смещенными, в то же время предложенный в работе ЕМ-алгоритм позволяет получить оценки угловых координат, для которых характерно значительно меньшее смещение относительно истинных значений параметров.

Структура статьи включает в себя: описание модели и формулировку задачи в разделе 2; представление ЕМ-алгоритма для оценивания угловых координат неподвижных источников сигналов в разделе 3; анализ результатов численного эксперимента в разделе 4; заключительные выводы в разделе 5.

\section{Описание модели и постановка задачи}
Предположим, имеется линейная равномерная антенная решетка, состоящая из $L$ сенсоров, которая принимает узкополосные сигналы, направленные из $K$ отдаленных источников, причем $K < L$. Этим источникам соответствуют угловые координаты (DoA) $\theta = [\theta_1, \ldots, \theta_{K}]^T$, практически не изменяющиеся во времени. По итогам измерений было получено $T$ снимков полученного сигнала, причем ввиду стохастических технических сбоев, связанных с сенсорами, часть таких снимков содержит помимо доступных данных пропуски. Матрица принятых сигналов (наблюдений) $X$ является результатом следующей модели наблюдений:
\begin{equation}\label{main_model}
X = \bA(\theta) S + N,
\end{equation}
где
\begin{equation*}
X=[X_1,\ldots, X_T], S=[S_1,\ldots, S_T], N=[N_1,\ldots, N_T],
\end{equation*}
и $X_t \in \Complex^{L}, S_t \in \Complex^{K}, N_t \in \Complex^{L}$ -- векторы-столбцы, соответствующие наблюдениям, сигналам и шумам в момент времени  $t = 1, \ldots, T$, $\bA(\theta)$ -- матрица направляющих векторов для линейной равномерной антенной решетки размера $L \times K$:
\begin{equation*}
\bA(\theta) = \begin{bmatrix}
1&1&\dots&1\\
e^{-2i\pi \frac{\text{d}}{\lambda}\sin(\theta_1)}& e^{-2i\pi \frac{\text{d}}{\lambda}\sin(\theta_2)}&\dots&e^{-2i\pi \frac{\text{d}}{\lambda}\sin(\theta_K)}\\
\dots&\dots&\ddots&\vdots\\
e^{-2i\pi (L-1) \frac{\text{d}}{\lambda}\sin(\theta_1)}& e^{-2i\pi (L-1) \frac{\text{d}}{\lambda}\sin(\theta_2)}&\dots&e^{-2i\pi (L-1) \frac{\text{d}}{\lambda}\sin(\theta_K)}\\
\end{bmatrix},
\end{equation*}
где $i$ -- мнимая единица, $d$ -- расстояние между сенсорами, $\lambda$ -- длина волны, $\theta_k$ -- угловая координата $k$-го источника, $k = 1, \ldots, K$. Через $x = [x_1,\ldots, x_T]$ будем обозначать реализацию случайной матрицы $X=[X_1,\ldots, X_T]$. Для каждого момента времени $t=1,\ldots,T$ определено непустое множество индексов, соответстветствующих доступным (для наблюдения) компонентам вектора $X_t$: $o_t \subseteq \{1, \ldots, L\}, \quad 1 \leq |o_t| \leq L$, и множество индексов, соответствующих пропускам в векторе $X_t$: $m_t \subset \{1, \ldots, L\}$, причем $o_t \cup m_t = \{1,\ldots,L\}, o_t \cap m_t = \varnothing,  \forall t \in \{1,\ldots,T\}$. Доступная и недоступная части вектора $X_t$ обозначаются через $X_{t,o_t} = ((X_t)_i)_{i \in o_t} \in \Complex{С}^{|o_t|}$ и $X_{t,m_t} = ((X_t)_i)_{i \in m_t} \in \Complex{С}^{|m_t|}$ соответственно. Таким образом, будем считать, что набор наблюдений $X$ состоит из доступной части $X_o = \{X_{t, o_t}\}_{t=1}^T$ и недоступной: $X_m = \{X_{t, m_t}\}_{t=1}^T$. Предполагается, что пропуски в наблюдениях $X_t$ являются абсолютно случайными, то есть относятся к типу MCAR по классификации Дональда Рубина ~\cite{Rubin2}. Вероятность возникновения таких пропусков не зависит от значений доступных для наблюдений или тех значений, что были пропущены. Также предполагается, что для реализации $x$ случайной матрицы $X$ расположение пропусков известно.\\

Cигналы, испускаемые источниками, шумы на сенсорах и наблюдения предполагаются стохастическими: $S_t \sim \CN(\mathbf{O}_{K \times 1},\bGamma)$, $N_t \sim \CN(\mathbf{O}_{L \times 1}, \bLambda)$, $ X_t \sim \CN(\mathbf{O}_{L \times 1}, \bA(\theta)\bGamma\bA(\theta)^* + \bLambda)$, $t=1,\ldots,T$, где $\CN$ соответствует комплексному нормальному распределению с нулевой псевдоковариацией. Шум и сигнал предполагаются независимыми между собой дискретными комплексными гауссовскими процессами с независимыми и одинаково распределенными отсчетами. Матрица $\bLambda$ предполагается диагональной. Требуется найти оптимальные оценки параметров $\Upsilon = (\theta, \bGamma)$.
\section{Описание алгоритма}
Воспользуемся EM-алгоритмом для того, чтобы оценить значения параметров $\Upsilon$,  $\Xi = (X_m, S)$ -- латентные переменные (недоступные значения и сигналы). Пусть $\Xi_t = (X_{t, m_t}, S_t)$ -- набор латентных переменных, соответствующих наблюдению $X_t$, причем для отдельных наблюдений $X_{t, m_t}=\varnothing$. Алгоритм состоит из двух шагов:
\begin{itemize}
\item  {\bf E-шаг:}  найти математическое ожидание полного правдоподобия с учетом апосториорного распределения латентных переменных:
$$
\Mexp_{q(\Xi)}[\log \Prob(X, S)],
$$
где 
$$
q(\Xi) = \Prob(X_m,S \mid X_o=x_o, \Upsilon^{(\tau-1)}),
$$
причем $\tau$ -- номер итерации ЕМ-алгоритма.
\item {\bf M-шаг:}
$$\Upsilon^{(\tau)}=\argmax_{\Upsilon \in \mathbb{R}^{K^2 + K}} \Mexp_{q(\Xi)}[\log \Prob(X, S)].
$$ 
\end{itemize}
\subsection{E-шаг}
Из построения модели наблюдений следует независимость и одинаковая распределенность наблюдений $X_t$, $t=1,\ldots,T$. Требуется найти математическое ожидание полного правдоподобия с учетом  текущей оценки параметров и апостериорного совместного распределения пропущенных значений в наблюдениях $X_m$ и сигналов $S$ 
\begin{equation*}
 \Mexp_{q(\Xi)}[\log \Prob(X, S)]= \Mexp_{(X_m,S) \mid X_o=x_o, \Upsilon^{(\tau-1)}}[\log \Prob(X, S)].
\end{equation*}

Преобразуем выражение, учитывая тот факт, что наблюдения являются независимыми и одинаково распределенными, и обозначив через $\Info$ доступную информацию $X_o=x_o, \Upsilon^{(\tau-1)}$  на итерации $\tau$  и через $q(\Xi_t)$ распределение $\Prob(X_{t,m_t},S_t \mid X_{t,o_t}=x_{t,o_t}, \Upsilon^{(\tau-1)})$:
\begin{equation*}
\begin{gathered}
 \Mexp_{q(\Xi)}[\log \Prob(X, S)] = \\
 \Mexp_{q(\Xi)}[\log \Prob(X\mid S) + \log \Prob(S)] = \\
\Big[\sum_{t=1}^T  \Mexp_{q(\Xi_t)} \log [\Prob(X_t \mid S_t)] + \sum_{t=1}^T \Mexp_{q(\Xi_t)}[\log \Prob(S_t)]\Big] = \\
- T \Big[\log|\bLambda| +\Tr \big( \bLambda^{-1} \Mexp_{q(\Xi)}[XX^*] \big) - 2 \Re \Tr\big(\bA^*(\theta) \bLambda^{-1} \Mexp_{q(\Xi)}[X S^*]\big) \\
+ \Tr(\bLambda^{-1} \bA(\theta) \Mexp_{q(\Xi)}[SS^*] \bA^*(\theta)) + \log |\bGamma| + \Tr(\bGamma^{-1} \Mexp_{q(\Xi)}[SS^*]) \Big],
\end{gathered}
\end{equation*}
где:
\begin{equation}
\Mexp_{q(\Xi)}[XX^*] = \frac{1}{T}\sum_{t=1}^T \Mexp_{q(\Xi)}[X_t X_t^* ], 
\end{equation}
\begin{equation}
\Mexp_{q(\Xi)}[S S^*] =  \frac{1}{T} \sum_{t=1}^T \Mexp_{q(\Xi)}[S_t S_t^*],
\end{equation}
\begin{equation}
\Mexp_{q(\Xi)}[X S^*] =  \frac{1}{T} \sum_{t=1}^T \Mexp_{q(\Xi)}[X_t S_t^*].
\end{equation}
Распределение $\Prob(X_{t,m_t},S_t \mid X_{t,o_t})$ задается однозначно следующими условиями ~\cite{SchreierScharf}:
\begin{enumerate}
\item
$S_t$ и $N_t$ независимы $\forall t \in \{1, \ldots, T \}$;
\item
$\bA(\theta)$ -- матрица линейного преобразования;
\item
$S_t, N_t$ --  комплексные гауссовские векторы,  которые имеют нулевую псевдоковариацию.
\end{enumerate}
На практике, не требуется напрямую находить ковариации для распределений $q(\Xi_t), t=1,...,T$, достаточно лишь найти усредненные условные вторые моменты $\Mexp_{q(\Xi)}[XX^*]$, $\Mexp_{q(\Xi)}[XS^*]$, $\Mexp_{q(\Xi)}[SS^*]$, используя формулы (2-4). \\
Рассмотрим случай, когда наблюдение $X_t$ является полностью доступным, т.е. $m_t = \varnothing$. Тогда, апостериорное распределение $\Prob(X_{t,m_t},S_t \mid \Info_t)$ сводится к распределению $\Prob(S_t \mid \Info_t)$, а условные моменты $\Mexp[X_t X_t^* \mid \Info_t]$, $\Mexp[X_t S_t^* \mid \Info_t]$, $\Mexp[S_t S_t^* \mid \Info_t]$ вычисляются по следующей схеме. Находим параметры апостериорного распределения $\Prob(S_t \mid \Info_t), t = 1,\ldots, T$:
\begin{equation*}
\left\{ \begin{aligned} 
\Mexp[S_t \mid \Info_t] &= \bGamma^*\bA^* \bR^{-1}x_t, \\
\cov(S_t \mid \Info_t) &= \bGamma - \bGamma^*\bA^*  \bR^{-1}\bA\bGamma.
\end{aligned} \right.
\end{equation*}
И вычисляем условные моменты следующим образом:
\begin{equation*}
\Mexp [X_t X_t^* \mid \Info_t] = \Mexp[X_t \mid \Info_t] \cdot \Mexp[X_t^* \mid \Info_t] = x_t x_t^*,
\end{equation*}
\begin{equation*}
\Mexp [X_t S_t^* \mid \Info_t] = \Mexp[X_t \mid \Info_t] \cdot \Mexp[S_t^* \mid \Info_t] = x_t \cdot \Mexp[S_t^* \mid \Info_t] = x_t \cdot x_t^* \cdot \bR^{-1} \bA \bGamma,
\end{equation*}
\begin{equation*}
\Mexp [S_t S_t^* \mid \Info_t] = \Mexp[S_t \mid \Info_t] \cdot \Mexp[S_t^* \mid \Info_t] + \cov(S_t \mid\Info_t). 
\end{equation*} \\
Теперь рассмотрим случай, когда наблюдение $X_t$ содержит пропуски, т.е. $m_t \ne \varnothing$.
Сначала найдем апостериорное распределение $\Prob(X_m \mid \Info)$. Для достижения этой цели, для каждой пары $ \{ (o_t, m_t): m_t \ne \varnothing \}$ создадим разбиение оценки ковариационной матрицы наблюдений $\bR$ на блоки, индуцированное этим разбиением множества индексов, оно имеет следующий вид:
\begin{equation*}
\bR =
\begin{pmatrix}
\bR_{o_t, o_t} & \bR_{o_t, m_t}\\
\bR_{m_t, o_t} & \bR_{m_t, m_t}
\end{pmatrix},
\end{equation*}
где каждый блок определяется как
\begin{equation*}
\bR_{a,b} = (\bR_{ij})_{i \in a, j \in b}.
\end{equation*}
Для каждого наблюдения, содержащего пропуски, требуется найти апостериорное распределение пропущенных значений, $\Prob(X_{t, m_t} \mid X_{t, o_t}=x_{t, o_t},\Upsilon^{(\tau-1)}), t=1,\ldots,T, m_t \ne \varnothing$ . Обозначим через $\Info_t$ доступную информацию $X_{t, o_t}=x_{t, o_t},\Upsilon^{(\tau-1)}$ на итерации $\tau$.
Параметры апостериорного распределения $\Prob(X_{t, m_t} \mid \Info_t), t=1,\ldots,T$ на итерации $\tau$ можно найти следующим образом ~\cite{LittleRubin}:
\begin{equation*}
\left\{ \begin{aligned} 
\Mexp[X_{t, m_t} \mid \Info_t] &= \bR_{m_t, o_t}\left(\bR_{o_t, o_t}\right)^{-1}\cdot x_{t, o_t}, \\
\cov(X_{t, m_t}\mid \Info_t) &= \bR_{m_t, m_t}-\bR_{m_t, o_t}\left(\bR_{o_t, o_t}\right)^{-1}\bR_{o_t, m_t},
\end{aligned} \right.
\end{equation*}
где $\bR_{o_t, o_t}= \bR_{o_t, o_t}^{(\tau-1)}$, 
$\bR_{o_t, m_t}= \bR_{o_t, m_t}^{(\tau-1)}$, 
$\bR_{m_t, o_t}= \bR_{m_t, o_t}^{(\tau-1)}$,
$\bR_{m_t, m_t}= \bR_{m_t, m_t}^{(\tau-1)}$.\\
Для каждого наблюдения $X_t$, содержащего пропуски, определим условные первый и второй начальный момент $\Mexp[X_t X_t^* \mid \Info_t]$:
\begin{equation*}
\begin{gathered}
\Mexp[X_t \mid \Info_t] = \Mexp \bigg[\begin{pmatrix} X_{t,o_t} \\ X_{t,m_t}\end{pmatrix}\Big| \Info_t\bigg] 
= \begin{pmatrix} x_{t,o_t} \\ \Mexp[X_{t,m_t} \mid \Info_t]\end{pmatrix},\\
\Mexp[X_t X_t^* \mid \Info_t] = \Mexp [X_t \mid \Info_t] \cdot \Mexp [X_t^* \mid \Info_t] + \cov (X_t \mid \Info_t) \\
= \Mexp [X_t \mid \Info_t] \cdot \Mexp [X_t^* \mid \Info_t] +
\begin{pmatrix}
\mathbf{O}_{o_t, o_t} & \mathbf{O}_{o_t, m_t} \\
\mathbf{O}_{m_t, o_t} & \cov(X_{t, m_t} \mid \Info_t) 
\end{pmatrix},
\end{gathered}
\end{equation*}
где $\mathbf{O}_{o_t, o_t}$, $\mathbf{O}_{o_t, m_t}$, $\mathbf{O}_{m_t, o_t}$ -- нулевые блочные матрицы. Разбиение указанной матрицы на четыре блока, три из которых состоят из нулей, индукцировано разбиением множества индексов $\{1,\ldots,L\}$ на множества $o_t, m_t$. \\
Параметры апостериорного распределения $\Prob(S_t \mid \Info_t), t = 1,\ldots, T$ можно найти следующим образом:
\begin{equation*}
\left\{ \begin{aligned} 
\Mexp[S_t \mid \Info_t] &= \bGamma^*\bA^* \bR^{-1}\Mexp [X_t \mid \Info_t], \\
\cov(S_t \mid \Info_t) &= \bGamma - \bGamma^*\bA^*  \bR^{-1}\bA\bGamma + \bGamma^*\bA^* \bR^{-1}\cov(X_t \mid \Info_t)\bR^{-1} \bA\bGamma,
\end{aligned} \right.
\end{equation*}
где $\bA=\bA(\theta^{(\tau-1)})$, $\bGamma=\bGamma^{(\tau-1)}$, $\bR=\bR^{(\tau-1)}$; эти же обозначения используются и далее, если не оговорено иное. Для вывода этих формул используются телескопическое свойство условного математического ожидания и закон полной дисперсии (подробности приведены в приложении Б). \\
Оценим $\Mexp [S_t S_t^* \mid \Info_t]$:
\begin{equation*}
\Mexp [S_t S_t^* \mid \Info_t] = \Mexp[S_t \mid \Info_t] \cdot \Mexp[S_t^* \mid \Info_t] + \cov(S_t \mid \Info_t),
\end{equation*}
Оценим $\Mexp [X_t S_t^* \mid \Info_t]$:
\begin{equation*}
\begin{gathered}
\Mexp [X_t S_t^* \mid \Info_t] = \Mexp[X_t X_t^* \mid \Info_t]\bR^{-1}\bA\bGamma.
\end{gathered}
\end{equation*}
\subsection{М-шаг}
Требуется найти наилучшую оценку параметров, решив следующую задачу оптимизации:
\begin{equation*}
\begin{gathered}
\Upsilon^{(\tau)}=\argmax_{\Upsilon \in \mathbb{R}^{K^2 + K}} \Mexp_{q(\Xi)}[\log \Prob(X, S)] = \\
\argmin_{\Upsilon \in \mathbb{R}^{K^2 + K}}  T \Big[\log|\bLambda| +\Tr \big( \bLambda^{-1} \Mexp_{q(\Xi)}[XX^*] \big) - 2 \Re \Tr\big(\bA^*(\theta) \bLambda^{-1} \Mexp_{q(\Xi)}[X S^*]\big) \\
+ \Tr(\bLambda^{-1} \bA(\theta) \Mexp_{q(\Xi)}[SS^*] \bA^*(\theta)) + \log |\bGamma| + \Tr(\bGamma^{-1} \Mexp_{q(\Xi)}[SS^*]) \Big].
\end{gathered}
\end{equation*}
Оценим угловые координаты источников $\theta$:
\begin{equation*}
\begin{gathered}
\theta^{(\tau)}= \argmin_{\theta \in \mathbb{R}^{K}}\mathcal{Q}_1(\theta|\theta^{(\tau-1)})  = \argmin_{\theta \in \mathbb{R}^{K}} \Big[- 2 \Re \Tr\big(\bA^*(\theta) \bLambda^{-1} \Mexp_{q(\Xi)}[X S^*]\big) \\
+ \Tr(\bLambda^{-1} \bA(\theta) \Mexp_{q(\Xi)}[S S^*] \bA^*(\theta)) \Big].
\end{gathered}
\end{equation*}
Эту задачу можно решить численно, подробности приведены в приложении В. \\
Оценим ковариацию сигналов $\bGamma$:
\begin{equation*}
\begin{gathered}
\bGamma^{(\tau)}= \argmin_{\bGamma \in \mathbb{R}^{K^2}} \mathcal{Q}_2(\bGamma \mid \bGamma^{(\tau-1)})
 =  \argmin_{\bGamma \in \mathbb{R}^{K^2}} T\bigg[ \log |\bGamma| + \Tr(\bGamma^{-1}\Mexp_{q(\Xi)}[S S^*])\bigg] = \Mexp_{q(\Xi)}[S S^*] .
\end{gathered}
\end{equation*}
Обновляем оценку ковариации наблюдений с учетом полученных оценок параметров:
\begin{equation*}
\bR^{(\tau)} = \bA(\theta^{(\tau)})\bGamma^{(\tau)}\bA^*(\theta^{(\tau)}) + \bLambda.
\end{equation*}

\section{Результаты численного эксперимента}

\subsection{Первый набор начальных условий}
Рассматриваются следующие начальные условия: $L=25, K=1, T=12$, 10 сенсоров неисправны, на каждом из таких сенсоров 50\% данных отсутствует, $\theta = [0.7] \approx [40.107^{\circ}]$. Мощность сигнала $\rho_1$, соответствующая первому источнику, равна -3dB. Мощность шума $\rho_{noise}$ равна 9dB.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{Screens/set1_angle1.png}
    \caption{График сходимости угловой координаты для первого набора начальных условий}
\end{figure}
%
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{Screens/set1_lkhd.png}
    \caption{График роста $\log \Prob(X_o \mid \Upsilon)$ для первого набора начальных условий}
\end{figure}
\clearpage
\subsection{Второй набор начальных условий}
Рассматриваются следующие начальные условия: $L=40, K=1, T=20$, 20 сенсоров неисправны, на каждом из таких сенсоров 50\% данных отсутствует, $\theta = [-0.5 0.7] \approx [-28.65^{\circ} 40.107^{\circ}]$. Мощность сигнала $\rho_1$, соответствующая первому источнику,  равна 0,8 dB. Мощность сигнала $\rho_2$, соответствующая второму источнику,  равна 1 dB. Мощность шума $\rho_{noise}$ равна 4 dB.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.6\textwidth]{Screens/set2_angle1.png}
    \caption{График сходимости угловой координаты для второго набора начальных условий}
\end{figure}
%
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.6\textwidth]{Screens/set2_angle2.png}
    \caption{График сходимости угловой координаты для второго набора начальных условий}
\end{figure}
%
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.6\textwidth]{Screens/set2_lkhd.png}
    \caption{График роста $\log \Prob(X_o \mid \Upsilon)$ для второго набора начальных условий}
\end{figure}
\clearpage
\section{Заключение}
\section*{\makebox[\linewidth][c]{Приложение А. Инициализация}}
Оценим ковариационную матрицу $\bR^{(0)}$ следующим образом: если $\sum_t \mathbf{I}_{x_t=x_{t,o_t}} \ge L$, оцениваем $\bR^{(0)}$ по выборке, образованной $x_t: x_t=x_{t,o_t}$. В противном случае используется следующий подход: пусть $x^l$ -- строка матрицы $X$, соответствующая сенсору $l, l=1,\ldots,L$, $x_o^l$ -- выборка, образованная доступными наблюдениями в строке $l$, $x_m^l$ -- выборка, образованная недоступными наблюдениями в строке $l$, $x_t^l$ -- компонента наблюдения $x_t$, соответствующая сенсору $l$. Для всех недоступных значений построим следующую оценку: если $x_t^l \in x_m^l, l = 1,\ldots, L,  t = 1,\ldots, T$, то $\hat{x}_t^l = \overline{x_o^l}$. Оцениваем ковариацию по такому оцененному набору $\hat{x}$:
\begin{equation*}
\bR^{(0)} = \cov(\hat{x}).
\end{equation*}
\\
Оценим вектор угловых координат источников $\theta^{(0)}$ следующим образом:
\begin{enumerate}
\item
Выберем число $\nu$, которое будет соответствовать первому компоненту вектора $\theta^{(0)}$:
\begin{equation*}
\nu \sim \mathcal{U}([-\pi;\pi]);
\end{equation*}
\item
Оценим компоненты вектора $\theta^{(0)}$ так:  $\theta^{(0)}_i = (\nu + (i-1)\cdot \frac{2\pi}{K})\, \text{mod} \, 2\pi, i = 1,\ldots,K$. При этом,  $a \, \text{mod} \, b = a - b \cdot \lfloor \frac{a}{b} \rfloor$.
\end{enumerate}
Альтернативный подход: используем $\bR^{(0)}$ для оценки  $\theta^{(0)}$ с помощью алгоритма MUSIC, используем эту оценку на первой итерации мультистарта ЕМ, на следующих используем оценки в окрестности указанной.\\
Начальную ковариацию сигналов $\bGamma^{(0)}$ получаем на основе метода наименьших квадратов с помощью $\theta^{(0)}, \bR^{(0)}$. Матрица $\bA(\theta^{(0)})$ -- матрица векторов направленности и представима в следующем виде:
\begin{equation*}
\bA(\theta^{(0)})=[\bA(\theta^{(0)}_1), \ldots, \bA(\theta^{(0)}_K)],
\end{equation*}
где $\bA(\theta^{(0)}_k)$ -- вектор направленности для источника $k$. Перед применением МНК каждый такой вектор отнормируем:
\begin{equation*}
\check{\bA}(\theta^{(0)}_k) = \frac{\bA(\theta^{(0)}_k)}{||\bA(\theta^{(0)}_k)||}, k=1,\ldots,K,
\end{equation*}
сформируем из них нормированную матрицу векторов направленности:
\begin{equation*}
\check{\bA}(\theta^{(0)})=[\check{\bA}(\theta^{(0)}_1), \ldots, \check{\bA}(\theta^{(0)}_K)],
\end{equation*}
применяем МНК для получения нормированной ковариации мощностей: 
\begin{equation*}
\check{\bGamma}^{(0)}=\mathcal{D}\big[\check{\bA}(\theta^{(0)})^{+} (\bR^{(0)}-\bLambda)(\check{\bA}(\theta^{(0)})^{+})^*\big],
\end{equation*}
где $\mathcal{D}\big[ \cdot \big]$ -- диагональное приближение матрицы, $(\cdot)^{+}$ -- псевдообратная матрица. Такой подход не гарантирует, что все элементы матрицы на главной диагонали будут неотрицательными, поэтому используем следующую коректировку: $\check{\bGamma}_{ll}^{(0)} = \max(\check{\bGamma}_{ll}^{(0)}, \varepsilon)$, $l=1,\ldots,L$. Переходим к ненормированной мощности:
\begin{equation*}
\bGamma_{ll}^{(0)}=\frac{\check{\bGamma}_{ll}^{(0)}}{||a(\theta^{(0)}_k)||_l}, l=1,\ldots,L.
\end{equation*}
Требуется гарантировать, что при выбранной инициализации на Е-шаге не будет получена условная ковариация сигналов, не являющаяся положительной определенной.
Вычисляем оценку условной ковариации сигналов, используя начальную оценку, и проверяем, выполняется ли свойство неотрицательной определенности:
\begin{equation*}
\bGamma^{(0)} - (\bGamma^{(0)})^* \bA(\theta^{(0)})^*  (\bR^{(0)})^{-1} \bA(\theta^{(0)})\bGamma^{(0)} \succeq 0,
\end{equation*} 
пока это свойство не выполняется, домножаем матрицу на 0.5: $\bGamma^{(0)} = 0.5 \cdot \bGamma^{(0)}$ и повторно проверяем выполненность вышеуказанного свойства.
\clearpage
\section*{\makebox[\linewidth][c]{Приложение Б. О корректности Е-шага}}
В рамках Е-шага необходимо вычислить $\Mexp[S_t \mid X_{t,o_t}=x_{t, o_t}]$,  $\cov(S_t \mid X_{t,o_t}=x_{t, o_t})$.
Вычисление $\Mexp[S_t \mid X_{t,o_t}=x_{t, o_t}]$ реализуется так:
\begin{equation*}
\begin{gathered}
\Mexp[S_t \mid X_{t,o_t}=x_{t, o_t}] =  \Mexp[\Mexp[S_t \mid X_t] \mid X_{t,o_t}=x_{t,o_t}]  \\ 
= \Mexp[\bGamma^* \bA^* \bR^{-1} X_t \mid X_{t,o_t}=x_{t, o_t}] = \bGamma^* \bA^* \bR^{-1} \Mexp[X_t \mid X_{t,o_t}=x_{t, o_t}].
\end{gathered}
\end{equation*}
Для нахождения $\cov(S_t \mid X_{t,o_t}=x_{t, o_t})$ может быть использовано равенство, называемое законом полной дисперсии (Law of total variance, ~\cite{Ross}):
\begin{equation*}
\cov(W) = \Mexp[\cov(W \mid Y)] + \cov(\Mexp[W \mid Y]).
\end{equation*}
Пусть $\mathcal{F}_Z=\sigma(Z)$: -- сигма-алгебра, порожденная $Z$. Тогда все числовые характеристики $W$, включая условные моменты и ковариации относительно $Y$, можно обусловить на $\mathcal{F}_Z$. И, соответственно, из закона полной ковариации следует:
\begin{equation*}
\cov(W \mid \mathcal{F}_Z) = \Mexp[\cov(W \mid Y,Z) \mid \mathcal{F}_Z] + \cov(\Mexp[W \mid Y,Z] \mid \mathcal{F}_Z).
\end{equation*}
Вектора $W$, $Y$, $Z$ предполагаются комплексными гауссовскими (случай круговой симметрии), зависимость между $Z$ и $Y$ линейная, $\sigma(Z) \subseteq \sigma(Y)$, предполагается, что $Z=CY$, где $C$ -- булев селектор.  Для линейной гауссовской модели знание $Z$ не уменьшает условную ковариацию $W \mid Y$ и не влияет на условное математическое ожидание $\Mexp[W \mid Y,Z]$, поскольку $Z$ не содержит никакой новой информации, которой не было бы в $Y$:
\begin{equation*}
\begin{gathered}
\cov(W \mid Y,Z)=\cov(W \mid Y), \\
\Mexp[W \mid Y,Z] = \Mexp[W \mid Y].
\end{gathered}
\end{equation*}
Получаем:
\begin{equation*}
\cov(W \mid \mathcal{F}_Z) = \Mexp[\cov(W \mid Y) \mid \mathcal{F}_Z] + \cov(\Mexp[W \mid Y] \mid \mathcal{F}_Z).
\end{equation*}
Если $Z=z$, имеем:
\begin{equation*}
\cov(W \mid Z=z) = \Mexp[\cov(W \mid Y) \mid Z=z] + \cov(\Mexp[W \mid Y] \mid Z=z).
\end{equation*}
В рамках исходной задачи по оцениванию DoA и ковариации сигналов, $W$ соответствует величине $S_t$,  $Y$ соответствует величине $X_t$, $Z$ соответствует величине $X_{t, o_t}$,  $z$ соответствует величине $x_{t, o_t}, t=1,\ldots,T$. \\
Получаем:
\begin{equation*}
\cov(S_t \mid X_{t,o_t}=x_{t, o_t}) = \Mexp[\cov(S_t \mid X_t) \mid X_{t,o_t}=x_{t, o_t}] + \cov(\Mexp[S_t \mid X_t]\mid X_{t,o_t}=x_{t, o_t}).
\end{equation*}
Преобразуем первое слагаемое, учитывая тот факт, что ковариация не зависит от реализации $X_{t, o_t}$.
\begin{equation*}
\Mexp[\cov(S_t \mid X_t) \mid X_{t, o_t}=x_{t, o_t}] = \cov(S_t \mid X_t) = \bGamma - \bGamma^* \bA^* \bR^{-1} \bA \bGamma.
\end{equation*}
Теперь преобразуем второе слагаемое:
\begin{equation*}
\begin{gathered}
\cov(\Mexp[S_t \mid X_t]\mid X_{t, o_t}=x_{t, o_t}) = \cov(\bGamma^* \bA^* \bR^{-1} X_t \mid X_{t, o_t}=x_{t, o_t})  \\ 
= \bGamma^* \bA^* \bR^{-1} \cov(X_t \mid X_{t, o_t}=x_{t, o_t}) \bR^{-1} \bA \bGamma.
\end{gathered}
\end{equation*}
Таким образом:
\begin{equation*}
\cov(S_t \mid X_{t,o_t}=x_{t, o_t}) = \bGamma - \bGamma^* \bA^* \bR^{-1} \bA \bGamma + \bGamma^* \bA^* \bR^{-1} \cov(X_t \mid X_{t, o_t}=x_{t, o_t}) \bR^{-1} \bA \bGamma.
\end{equation*}
Для полных наблюдений $X_{t, o_t} = X_t$, и, соответственно:
\begin{equation*}
\cov(\Mexp[S_t \mid X_t] \mid X_{t, o_t}=x_{t, o_t})  = \cov(\Mexp[S_t \mid X_t]\mid X_t=x_t) = 0. 
\end{equation*}
О выводе смешанных вторых моментов $\Mexp[X_t S_t^* \mid X_t=x_t]$ и $\Mexp[X_t S_t^* \mid X_{t, o_t}=x_{t, o_t}]$:
\begin{equation*}
\Mexp[X_t S_t^* \mid X_t] = X_t \Mexp[S_t^* \mid X_t] = X_t (\bGamma^* \bA^* \bR^{-1} X_t)^* = X_t X_t^* \bR^{-1} \bA \bGamma,
\end{equation*}
\begin{equation*}
\begin{gathered}
\Mexp[X_t S_t^* \mid X_{t, o_t}=x_{t, o_t}] = \Mexp[X_t \Mexp[S_t^* \mid X_t] \mid X_{t, o_t}=x_{t, o_t}] = \Mexp[X_t  (\bGamma^* \bA^* \bR^{-1} X_t)^*  \mid X_{t, o_t}=x_{t, o_t}] \\ 
= \Mexp[X_t X_t^* \bR^{-1} \bA \bGamma \mid X_{t, o_t}=x_{t, o_t}] = \Mexp[X_t X_t^* \mid X_{t, o_t}=x_{t, o_t}] \bR^{-1} \bA \bGamma
\end{gathered}
\end{equation*}
В приведенных выше переходах используется закон полного условного математического ожидания и уже полученное выражение для $\Mexp[S_t^* \mid X_t]$.
\clearpage
\section*{\makebox[\linewidth][c]{Приложение В. О реализации М-шага}}
Если антенная решетка является равномерной и линейной, удобно искать оптимальный $u=\sin(\theta)$, а затем находить $\theta$ как $\arcsin(u)$. Соответственно минимизации подлежит функция
\begin{equation*}
\mathcal{Q}_1(u|u^{(\tau-1)}) = - 2 \Re \Tr\big(\widetilde{\bA}^*(u) \bLambda^{-1} \Mexp_{q(\Xi)}[X S^*]\big) + \Tr(\bLambda^{-1} \widetilde{\bA}(u) \Mexp_{q(\Xi)}[S S^*] \widetilde{\bA}^*(u)),
\end{equation*}
где
\begin{equation*}
{\small
\widetilde{\bA}(u) = \begin{bmatrix}
1&1&\dots&1\\
e^{-2j\pi \frac{\text{d}}{\lambda}u_1}& e^{-2j\pi \frac{\text{d}}{\lambda}u_2}&\dots&e^{-2j\pi \frac{\text{d}}{\lambda}u_K}\\
\dots&\dots&\ddots&\vdots\\
e^{-2j\pi (L-1) \frac{\text{d}}{\lambda}u_1}& e^{-2j\pi (L-1) \frac{\text{d}}{\lambda}u_2}&\dots&e^{-2j\pi (L-1) \frac{\text{d}}{\lambda}u_K}\\
\end{bmatrix}.
}
\end{equation*}
Для ускорения оптимизации можно оптимизировать не $\mathcal{Q}_1(u \mid u^{(\tau-1)})$, а суррогатную функцию $\mathcal{G}(u \mid u^{(\tau-1)})$, построенную так:
\begin{equation*}
\mathcal{G}(u \mid u^{(\tau-1)}) = \mathcal{Q}_1(u^{(\tau-1)} \mid u^{(\tau-1)}) + \nabla \mathcal{Q}_1(u^{(\tau-1)} \mid u^{(\tau-1)})^T (u-u^{(\tau-1)}) + \frac{1}{2} (u-u^{(\tau-1)})^T \bH (u-u^{(\tau-1)}),
\end{equation*}
где
\begin{equation*}
{\small
\bH = \begin{bmatrix}
\max(|\nabla_1 \mathcal{Q}_1(u^{(\tau-1)} \mid u^{(\tau-1)})|,\varepsilon)&0&\dots&0\\
0& \max(|\nabla_2 \mathcal{Q}_1(u^{(\tau-1)} \mid u^{(\tau-1)})|,\varepsilon)&\dots&0\\
\dots&\dots&\ddots&\vdots\\
0& 0&\dots& \max(|\nabla_K \mathcal{Q}_1(u^{(\tau-1)} \mid u^{(\tau-1)})|,\varepsilon)\\
\end{bmatrix},
}
\end{equation*}
причем $\nabla_1 \mathcal{Q}_1(u^{(\tau-1)}\mid u^{(\tau-1)})$ -- $i$-я компонента градиента $\mathcal{Q}_1(u^{(\tau-1)}\mid u^{(\tau-1)})$. Диагональная аппроксимация $\bH$ предотвращает слишком маленькие или слишком большие шаги, сохраняя численную стабильность.
Теперь эту суррогатную функцию надо минимизировать (или, эквивалентно, найти направление шага $\rho^{(\tau)}=u-u^{(\tau-1)}$):
\begin{equation*}
\min_{u \in \mathbb{R}^{K}} \mathcal{G}(u \mid u^{(\tau-1)}).
\end{equation*}
Подставим $u = u^{(\tau-1)}+\rho$:
\begin{equation*}
\mathcal{G}(u^{(\tau-1)} + \rho \mid u^{(\tau-1)}) = \mathcal{Q}_1(u^{(\tau-1)}\mid u^{(\tau-1)}) + \nabla \mathcal{Q}_1 (u^{(\tau-1)}\mid u^{(\tau-1)})^T\rho + \frac{1}{2}\rho^T\bH\rho.
\end{equation*}
Для нахождения минимума берем градиент по $\rho$:
\begin{equation*}
\nabla_\rho \mathcal{G}(u^{(\tau-1)} + \rho\mid u^{(\tau-1)}) =  \nabla \mathcal{Q}_1(u^{(\tau-1)}\mid u^{(\tau-1)})   + \bH\rho,
\end{equation*}
при $\nabla_\rho \mathcal{G}=0$ будет выполняться:
\begin{equation*}
0 =  \nabla \mathcal{Q}_1(u^{(\tau-1)}\mid u^{(\tau-1)})   + \bH\rho,
\end{equation*}
решаем относительно $\rho^{(\tau)}$:
\begin{equation*}
\rho^{(\tau)} = -\bH^{-1}  \nabla \mathcal{Q}_1(u^{(\tau-1)} \mid u^{(\tau-1)}).
\end{equation*}
Важно заметить, что производная по направлению $\nabla \mathcal{Q}_1(u^{(\tau-1)} \mid u^{(\tau-1)})^T \rho^{(\tau)}$ принимает вид:
\begin{equation*}
\nabla \mathcal{Q}_1(u^{(\tau-1)}\mid u^{(\tau-1)})^T \rho^{(\tau)} = - \nabla \mathcal{Q}_1(u^{(\tau-1)}\mid u^{(\tau-1)})^T \bH^{-1}  \nabla \mathcal{Q}_1(u^{(\tau-1)}\mid u^{(\tau-1)}),
\end{equation*} 
и поскольку $\bH$ -- положительно определенная матрица, производная по направлению может принимать лишь неположительные значения, а значит направление $\rho^{(\tau)}$ соответствует невозрастанию функции ~\cite{NocedalWright}.
Далее используется backtracking line search в сочетании с проекцией на допустимое множество:
\begin{equation*}
\tilde u^{(\tau)} = u^{(\tau-1)} + \alpha_m \rho^{(\tau)}, \qquad
u^{(\tau)} = \Pi_{\mathcal{U}}(\tilde u^{(\tau)}),
\end{equation*}
где $\Pi_{\mathcal{U}}$ — евклидова проекция на множество
\[
\mathcal{U} = [-1,1]^L =
\{u \in \mathbb{R}^L \mid \|u\|_\infty \le 1\},
\]
задаваемая покомпонентным ограничением:
\[
(\Pi_{\mathcal{U}}(x))_i = \min\{1,\max\{-1,x_i\}\}.
\]
Параметр шага $\alpha_m$ выбирается как первое
$\alpha = \alpha_0 \beta^m$, $m \in \mathbb{N}$, такое что
\[
\mathcal{Q}_1\bigl(u^{(\tau)} \mid u^{(\tau-1)}\bigr)
\le
\mathcal{Q}_1\bigl(u^{(\tau-1)} \mid u^{(\tau-1)}\bigr),
\]
при фиксированных $\alpha_0 > 0$ и $\beta \in (0,1)$. \\
В рамках программного комплекса реализуется мультистарт для поиска оптимального $u$ на М-шаге. Указанный выше оптимизационный алгоритм запускается не только из точки $u^{(\tau-1)}$, но и из случайно выбранных допустимых точек, в том числе находящихся в окрестности $u^{(\tau-1)}$. Для выбранной начальной точки с индексом $j$, обновление оценки синусов угловых координат $\widetilde{u}^{(j)}$  может быть принято, только если:
\begin{enumerate}
\item
$\mathcal{Q}_1(\widetilde{u}^{(j)}\mid u^{(\tau-1)}) > \mathcal{Q}_1(u^{(\tau-1)}\mid u^{(\tau-1)})$,
\item
 $\mathcal{Q}_1(\widetilde{u}^{(j)}\mid u^{(\tau-1)}) > \mathcal{Q}_1(\min(\widetilde{u}^{(i)}: 1 \le i \le j-1)\mid u^{(\tau-1)})$, \text{при $j>1$}.
\end{enumerate}  
Выбранный подход корректен ввиду следующих соображений:
\begin{enumerate}
\item
Функция $\mathcal{Q}_1(u \mid u^{(\tau-1)})$  дифференцируема, поскольку $\widetilde{\bA}(u)$ состоит из экспонент вида $e^{j \pi m u_k}, k = 1, \ldots, K, m \in \mathbb{R}$, т.е. гладких функций, умножение подобных матриц на фиксированные матрицы, взятие следа, взятие действительной части не нарушают гладкость.
\item
Функция $\mathcal{Q}_1(u \mid u^{(\tau-1)})$  ограничена снизу, поскольку вектор $u$ принадлежит компакту $\mathcal{U}$, матрицы, определяющие вид функции $\mathcal{Q}_1(u \mid u^{(\tau-1)})$ и отличные от $\widetilde{\bA}(u)$, являются фиксированными, след от произведения таких матриц, принимает лишь конечные значения.
\item
Градиент $\nabla \mathcal{Q}_1(u \mid u^{(\tau-1)})$ не принимает нулевые значения для любых $u \in \mathcal{U}$, поскольку $ \mathcal{Q}_1(u \mid u^{(\tau-1)}) $ -- гладкая функция, не сводящаяся к константе.
\end{enumerate}
Оценим ковариацию сигналов $\bGamma$:
\begin{equation*}
\bGamma^{(\tau)}= \argmin_{\bGamma} \mathcal{Q}_2(\bGamma \mid \bGamma^{(\tau-1)}) = \argmin_{\bGamma} T\bigg[ \log |\bGamma| + \Tr(\bGamma^{-1}\Mexp_{q(\Xi)}[S S^*])\bigg].
\end{equation*}
Определим точку, где производная данной функции принимает значение 0, и, таким образом, находим минимум функции:
\begin{equation*}
\begin{gathered}
\frac{\partial}{\partial \bGamma}\log (\Det (\bGamma)) = \bGamma^{-1}, \\
\frac{\partial}{\partial \bGamma}\Tr(\bGamma^{-1}\Mexp_{q(\Xi)}[S S^*])= -\bGamma^{-1}\Mexp_{q(\Xi)}[S S^*]\bGamma^{-1}, \\
\frac{\partial \mathcal{Q}_2(\bGamma)}{\partial \bGamma} = \bGamma^{-1}-\bGamma^{-1}\Mexp_{q(\Xi)}[S S^*]\bGamma^{-1}.
\end{gathered}
\end{equation*}
Приравняем производную к нулю (функция по $\bGamma$ выпукла):
\begin{equation*}
\mathbf{O} = \bGamma^{-1}-\bGamma^{-1}\Mexp_{q(\Xi)}[S S^*]\bGamma^{-1} \Rightarrow \bGamma^{(\tau)} = \Mexp_{q(\Xi)}[S S^*].
\end{equation*}
Ввиду того, что М-шаг не максимизирует, а лишь улучшает УМО полного правдоподобия, представленный алгоритм является не точным ЕМ, а обобщенным (Generalized) EM.

\begin{thebibliography}{9}
%
\bibitem{KrimViberg}
Krim H., Viberg M. Two decades of array signal processing research: the parametric approach. {\em IEEE Signal Processing Magazine}, 1996,  Vol. 13, No. 4, pp. 67–94.
\bibitem{Godara}
Godara L. C. Application of antenna arrays to mobile communications. Part II: Beam-forming and direction-of-arrival considerations. {\em Proceedings of the IEEE}, 1997, Vol. 85, No. 8, pp. 1195-1245.
\bibitem{DempsterRubin}
Dempster A. P., Laird N. M., Rubin D. B. Maximum likelihood from incomplete data via the EM algorithm. {\em Journal of the Royal Statistical Society: Series B (Methodological)}, 1977,  Vol. 39, No. 1, pp. 1-38.
\bibitem{MengRubin}
Meng X.-L., Rubin D. B. Maximum likelihood estimation via the ECM algorithm: a general framework. {\em Biometrika}, 1993, Vol. 80, No. 2, pp. 267-278.
\bibitem{FesslerHero}
Fessler J. A., Hero A. O. Space-alternating generalized expectation-maximization algorithm. {\em IEEE Transactions on Signal Processing}, 1994, Vol. 42, No. 10, pp. 2664-2677. 
\bibitem{MillerFuhrmann}
Miller M. I., Fuhrmann D. R. Maximum‑likelihood narrow‑band direction finding and the EM algorithm.  {\em IEEE Transactions on Acoustics, Speech, and Signal Processing}, 1990, Vol. 38, No. 9,  pp. 1560-1577.
\bibitem{FederWeinstein}
Feder M., Weinstein E. Parameter estimation of superimposed signals using the EM algorithm. {\em IEEE Transactions on Acoustics, Speech, and Signal Processing}, 1988, Vol. 36, No. 4, pp. 477-489.
\bibitem{GongLyu2021}
Gong M.-Y., Lyu B. Alternating maximization and the EM algorithm in maximum‑likelihood direction finding. {\em IEEE Trans. Veh. Technol.}, 2021, Vol. 70, No. 10, pp. 9634-9645. 
\bibitem{GongLyu2023}
Gong M.-Y., Lyu B. EM and SAGE Algorithms for DOA Estimation in the Presence of Unknown Uniform Noise. {\em Sensors}, 2023, Vol. 23, No. 10.
\bibitem{GongLyu2023second}
Gong, M.-Y.; Lyu, B. Stochastic Maximum Likelihood Direction Finding in the Presence of Nonuniform Noise Fields. {\em Electronics}, 2023, Vol. 12, No. 10.
\bibitem{Schwarz}
Schwartz O., Dorfan Y., Taseska M., Habets E. A. P., Gannot S. DOA estimation in noisy environment with unknown noise power using the EM algorithm. {\em 2017 Hands-free Speech Communications and Microphone Arrays (HSCMA)},  San Francisco, CA, USA, 2017,  pp. 86-90.
\bibitem{Xunxue}
Xunxue C., Kegen Y., Songsheng L. Direction Finding for Transient Acoustic Source Based on Biased TDOA Measurement. {\em IEEE Transactions on Instrumentation and Measurement}, 2016, Vol. 65, No. 11, pp.2442-2453.
\bibitem{Wen}
Wen F., Liu W., Lu X. Distributed Expectation-Maximization Algorithm for DOA Estimation in Wireless Sensor Networks. {\em 2015 IEEE International Conference on Digital Signal Processing (DSP)}, Singapore, 2015, pp. 944-947.
\bibitem{Gimenez-Febrer}
Gimenez-Febrer P., Pages-Zamora A., Lopez-Valcarce R. Online EM-based distributed estimation in sensor networks with faulty nodes. {\em 2016 24th European Signal Processing Conference (EUSIPCO)}, Budapest, Hungary, 2016, pp. 175-179.
\bibitem{Rubin2}
Rubin, D.B. Inference and missing data. {\em Biometrika}, Vol. 63, No. 3, pp. 581-592.
\bibitem{SchreierScharf}
Schreier P. J., Scharf L. L. {\em Statistical Signal Processing of Complex-Valued Data: The Theory of Improper and Noncircular Signals.} Cambridge: Cambridge University Press, 2010. 309p., pp. 41-42.
\bibitem{LittleRubin}
Little R. J. A., Rubin D. B. {\em Statistical analysis with missing data 3rd ed.} Hoboken, N. J.: Wiley, 2019, 462p.
\bibitem{Ross}
Ross S. M. {\em A first course in probability. 8th ed.} Upper Saddle River, N. J.: Prentice Hall, 2010. 640p., p. 348.
\bibitem{NocedalWright}
Nocedal J., Wright S.J. {\em Numerical Optimization. 2nd ed.} New York: Springer, 2006. 664p., pp. 10-37.
\begin{comment}
\bibitem{Louis}
Louis T. A. Finding the observed information matrix when using the EM algorithm. 
{\em Journal of the Royal Statistical Society: Series B (Methodological)}, 1982, Vol. 44, No. 2, pp. 226–233.
\bibitem{Wu}
Wu C. F. J. On the convergence properties of the EM algorithm. {\em The Annals of Statistics}, 1983, Vol. 11, No. 1,  pp. 95–103.
\bibitem{Tang}
Tang G., Bhaskar B., Shah P., Recht B. Compressed sensing off the grid. {\em IEEE Trans. Information Theory}, 2013, Vol. 59, No. 11, pp. 7465–7490.
\end{comment}
\end{thebibliography}
\end{document}

